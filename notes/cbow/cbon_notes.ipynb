{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gi-IIUgYjVnD"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Here’s a brief explanation of the libraries you've imported:\n",
        "\n",
        "1. **`numpy` (imported as `np`)**:\n",
        "   - A fundamental library for numerical computing in Python.\n",
        "   - It provides support for large multi-dimensional arrays and matrices, along with mathematical functions to operate on these arrays (e.g., element-wise operations).\n",
        "\n",
        "2. **`re`**:\n",
        "   - The regular expression (regex) library in Python.\n",
        "   - It is used to search, match, and manipulate text using patterns, making it useful for tasks like pattern matching and string parsing.\n",
        "\n",
        "3. **`pandas` (imported as `pd`)**:\n",
        "   - A powerful library for data manipulation and analysis.\n",
        "   - It provides data structures like DataFrame and Series to handle structured data, perform data cleaning, and analyze datasets efficiently.\n",
        "\n",
        "These libraries are commonly used together in data analysis tasks."
      ],
      "metadata": {
        "id": "EJGeRWhIjZru"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = data.split('.')\n",
        "sentences"
      ],
      "metadata": {
        "id": "-g2DHGngjc3Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The code `sentences = data.split('.')` does the following:\n",
        "\n",
        "1. **`data.split('.')`**:\n",
        "   - Splits the string `data` into smaller parts (substrings) wherever there is a period (`.`).\n",
        "   - This creates a list of sentences, assuming each sentence ends with a period.\n",
        "\n",
        "2. **`sentences`**:\n",
        "   - Stores the resulting list of sentence-like segments from `data`.\n",
        "\n",
        "After running this code, `sentences` will contain each sentence from `data` as a separate item in a list."
      ],
      "metadata": {
        "id": "v5L41lFMjxCB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clean_sent=[]\n",
        "for sentence in sentences:\n",
        "    if sentence==\"\":\n",
        "        continue\n",
        "    sentence = re.sub('[^A-Za-z0-9]+', ' ', (sentence))\n",
        "    sentence = re.sub(r'(?:^| )\\w (?:$| )', ' ', (sentence)).strip()\n",
        "    sentence = sentence.lower()\n",
        "    clean_sent.append(sentence)\n",
        "\n",
        "clean_sent"
      ],
      "metadata": {
        "id": "e3AWMZgJj4m6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here’s a breakdown of what this code does:\n",
        "\n",
        "1. **`clean_sent = []`**:\n",
        "   - Creates an empty list `clean_sent` to store cleaned sentences.\n",
        "\n",
        "2. **`for sentence in sentences:`**:\n",
        "   - Iterates through each sentence in the `sentences` list.\n",
        "\n",
        "3. **`if sentence == \"\": continue`**:\n",
        "   - Skips any empty strings in `sentences` to avoid processing blank entries.\n",
        "\n",
        "4. **Cleaning the sentence**:\n",
        "   - **`re.sub('[^A-Za-z0-9]+', ' ', sentence)`**:\n",
        "     - Removes any characters that are not letters (`A-Za-z`) or numbers (`0-9`), replacing them with a space.\n",
        "   - **`re.sub(r'(?:^| )\\w (?:$| )', ' ', sentence).strip()`**:\n",
        "     - Removes any single characters (like \"a\" or \"I\") that are surrounded by spaces or appear at the beginning or end of the sentence.\n",
        "     - `strip()` removes any leading or trailing whitespace.\n",
        "   - **`sentence.lower()`**:\n",
        "     - Converts the sentence to lowercase.\n",
        "\n",
        "5. **`clean_sent.append(sentence)`**:\n",
        "   - Appends the cleaned, lowercase version of the sentence to the `clean_sent` list.\n",
        "\n",
        "After the loop, `clean_sent` contains the cleaned, processed version of each sentence from `sentences`."
      ],
      "metadata": {
        "id": "aZFpIekZkH-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer"
      ],
      "metadata": {
        "id": "EckE1qbZkOPS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The line `from tensorflow.keras.preprocessing.text import Tokenizer` does the following:\n",
        "\n",
        "- **`tensorflow.keras.preprocessing.text`**:\n",
        "  - A module within TensorFlow’s Keras API focused on preparing and preprocessing text data for machine learning models.\n",
        "\n",
        "- **`Tokenizer`**:\n",
        "  - A class that converts text data into sequences of tokens (e.g., words or subwords).\n",
        "  - It creates a vocabulary based on the input text and assigns each unique word an integer ID.\n",
        "  - Useful for tasks like text classification, sentiment analysis, or natural language processing.\n",
        "\n",
        "By importing `Tokenizer`, you can easily preprocess and tokenize text data, transforming it into numerical format suitable for neural networks."
      ],
      "metadata": {
        "id": "Dl6Zu-XrkbaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(clean_sent)\n",
        "sequences = tokenizer.texts_to_sequences(clean_sent)\n",
        "print(sequences)"
      ],
      "metadata": {
        "id": "FOOq62TMkcTl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here’s what each line of this code does:\n",
        "\n",
        "1. **`tokenizer = Tokenizer()`**:\n",
        "   - Initializes a `Tokenizer` object, which will be used to tokenize (convert words into integer sequences) the text.\n",
        "\n",
        "2. **`tokenizer.fit_on_texts(clean_sent)`**:\n",
        "   - Analyzes the list `clean_sent` and builds a vocabulary based on the unique words found in the sentences.\n",
        "   - Each word is assigned a unique integer ID based on its frequency, with more common words receiving lower integer IDs.\n",
        "\n",
        "3. **`sequences = tokenizer.texts_to_sequences(clean_sent)`**:\n",
        "   - Converts each sentence in `clean_sent` into a sequence of integers.\n",
        "   - Each integer represents a word from the vocabulary created by the tokenizer.\n",
        "   - For example, if \"the\" is the most frequent word, it might be encoded as `1` throughout the sequences.\n",
        "\n",
        "4. **`print(sequences)`**:\n",
        "   - Outputs the list `sequences`, where each sentence in `clean_sent` is now represented as a list of integers.\n",
        "\n",
        "After running this code, `sequences` contains the tokenized integer representations of each sentence from `clean_sent`."
      ],
      "metadata": {
        "id": "m_ILszm_kjNd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index_to_word = {}\n",
        "word_to_index = {}\n",
        "\n",
        "for i, sequence in enumerate(sequences):\n",
        "#     print(sequence)\n",
        "    word_in_sentence = clean_sent[i].split()\n",
        "#     print(word_in_sentence)\n",
        "\n",
        "    for j, value in enumerate(sequence):\n",
        "        index_to_word[value] = word_in_sentence[j]\n",
        "        word_to_index[word_in_sentence[j]] = value\n",
        "\n",
        "print(index_to_word, \"\\n\")\n",
        "print(word_to_index)"
      ],
      "metadata": {
        "id": "Fbya1OR-kuGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here’s what this code does:\n",
        "\n",
        "1. **`index_to_word = {}` and `word_to_index = {}`**:\n",
        "   - Creates two dictionaries:\n",
        "     - `index_to_word`: Maps each integer (token) to its corresponding word.\n",
        "     - `word_to_index`: Maps each word to its corresponding integer (token).\n",
        "\n",
        "2. **Loop through sequences**:\n",
        "   - **`for i, sequence in enumerate(sequences):`**:\n",
        "     - Iterates over each `sequence` in `sequences`, with `i` as the index of the sentence.\n",
        "\n",
        "3. **Token-to-Word Mapping**:\n",
        "   - **`word_in_sentence = clean_sent[i].split()`**:\n",
        "     - Splits the original sentence in `clean_sent[i]` (a cleaned, lowercase version of the sentence) into words.\n",
        "\n",
        "4. **Inner loop to populate dictionaries**:\n",
        "   - **`for j, value in enumerate(sequence):`**:\n",
        "     - Iterates through each integer `value` in `sequence`, with `j` as the index of the word in the sentence.\n",
        "   - **`index_to_word[value] = word_in_sentence[j]`**:\n",
        "     - Adds a key-value pair to `index_to_word`, mapping each integer `value` to the corresponding word in the sentence.\n",
        "   - **`word_to_index[word_in_sentence[j]] = value`**:\n",
        "     - Adds a key-value pair to `word_to_index`, mapping each word to its integer `value`.\n",
        "\n",
        "5. **`print(index_to_word, \"\\n\")` and `print(word_to_index)`**:\n",
        "   - Outputs the dictionaries:\n",
        "     - `index_to_word`: Maps tokens to their respective words.\n",
        "     - `word_to_index`: Maps words to their respective tokens.\n",
        "\n",
        "After running this code, you’ll have two dictionaries:\n",
        "- `index_to_word`: Allows you to decode tokenized sentences back into words.\n",
        "- `word_to_index`: Allows you to convert words into their tokenized (integer) form."
      ],
      "metadata": {
        "id": "e21PWD1Vk1HI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "emb_size = 10\n",
        "context_size = 2\n",
        "\n",
        "contexts = []\n",
        "targets = []\n",
        "\n",
        "for sequence in sequences:\n",
        "    for i in range(context_size, len(sequence) - context_size):\n",
        "        target = sequence[i]\n",
        "        context = [sequence[i - 2], sequence[i - 1], sequence[i + 1], sequence[i + 2]]\n",
        "#         print(context)\n",
        "        contexts.append(context)\n",
        "        targets.append(target)\n",
        "print(contexts, \"\\n\")\n",
        "print(targets)"
      ],
      "metadata": {
        "id": "kgRXQlCUlKl_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code is preparing data for a word embedding model by creating \"contexts\" and \"targets\" for each word in each sentence. Here's a breakdown:\n",
        "\n",
        "1. **Define parameters**:\n",
        "   - **`vocab_size = len(tokenizer.word_index) + 1`**:\n",
        "     - `vocab_size` is the total number of unique words in the vocabulary, plus 1 to account for padding or indexing starting at 1.\n",
        "   - **`emb_size = 10`**:\n",
        "     - `emb_size` is the size of the word embedding vector, which determines how many features represent each word.\n",
        "   - **`context_size = 2`**:\n",
        "     - `context_size` defines the window around each target word (in this case, 2 words on each side).\n",
        "\n",
        "2. **Initialize `contexts` and `targets` lists**:\n",
        "   - `contexts` will store the words around each target word.\n",
        "   - `targets` will store each target word.\n",
        "\n",
        "3. **Loop through each sequence in `sequences`**:\n",
        "   - For each sequence (sentence represented by integer tokens):\n",
        "     - **`for i in range(context_size, len(sequence) - context_size):`**:\n",
        "       - Loops through the sequence, excluding the first and last `context_size` tokens to ensure context words are available.\n",
        "     - **`target = sequence[i]`**:\n",
        "       - Sets the `target` word at position `i`.\n",
        "     - **`context = [sequence[i - 2], sequence[i - 1], sequence[i + 1], sequence[i + 2]]`**:\n",
        "       - Creates a `context` list containing 4 tokens (2 before and 2 after the target).\n",
        "\n",
        "4. **Append `context` and `target` to their respective lists**:\n",
        "   - `contexts.append(context)`: Adds the list of context words to `contexts`.\n",
        "   - `targets.append(target)`: Adds the target word to `targets`.\n",
        "\n",
        "5. **Print `contexts` and `targets`**:\n",
        "   - Outputs the lists:\n",
        "     - `contexts`: Each entry contains the context words around a target word.\n",
        "     - `targets`: Each entry is the target word corresponding to each context.\n",
        "\n",
        "This setup is often used in word embedding models like Skip-gram, where the goal is to predict the target word based on its surrounding context."
      ],
      "metadata": {
        "id": "I6sTkl0jn9OJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    words = []\n",
        "    target = index_to_word.get(targets[i])\n",
        "    for j in contexts[i]:\n",
        "        words.append(index_to_word.get(j))\n",
        "    print(words,\" -> \", target)"
      ],
      "metadata": {
        "id": "47ltuBaRoHJL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code prints sample context-target pairs for the first few items in the `contexts` and `targets` lists, converting integer tokens back to words using the `index_to_word` dictionary.\n",
        "\n",
        "1. **Loop through the first 5 items**:\n",
        "   - **`for i in range(5):`**:\n",
        "     - Iterates through the first five entries in `contexts` and `targets` to print examples.\n",
        "\n",
        "2. **Initialize `words`**:\n",
        "   - **`words = []`**:\n",
        "     - Creates an empty list to store context words as actual words (rather than tokens).\n",
        "\n",
        "3. **Get the target word**:\n",
        "   - **`target = index_to_word.get(targets[i])`**:\n",
        "     - Retrieves the word corresponding to the `target` token at `targets[i]` using `index_to_word` dictionary.\n",
        "\n",
        "4. **Convert context tokens to words**:\n",
        "   - **`for j in contexts[i]:`**:\n",
        "     - Iterates through each token in `contexts[i]`.\n",
        "   - **`words.append(index_to_word.get(j))`**:\n",
        "     - Retrieves the word corresponding to each context token `j` and appends it to `words`.\n",
        "\n",
        "5. **Print context and target**:\n",
        "   - **`print(words, \" -> \", target)`**:\n",
        "     - Outputs the context words (`words`) and the target word (`target`) in the format `['context_word1', 'context_word2', ...] -> target_word`.\n",
        "\n",
        "This printout helps verify that context-target pairs are correctly formed. Each line represents a target word and its context, showing the words instead of numerical tokens."
      ],
      "metadata": {
        "id": "mdTiDmdEoRCM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(contexts)\n",
        "Y = np.array(targets)"
      ],
      "metadata": {
        "id": "K_aRE4usobLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code converts the `contexts` and `targets` lists into NumPy arrays:\n",
        "\n",
        "1. **`X = np.array(contexts)`**:\n",
        "   - Converts `contexts` (a list of context word tokens) into a NumPy array `X`.\n",
        "   - This allows for easier handling and efficient processing in machine learning models.\n",
        "\n",
        "2. **`Y = np.array(targets)`**:\n",
        "   - Converts `targets` (a list of target word tokens) into a NumPy array `Y`.\n",
        "\n",
        "After running this, `X` contains the context tokens for each target word in a structured array format, and `Y` contains the target tokens. This format is ready for input into machine learning models, where `X` would typically serve as the input features, and `Y` as the labels."
      ],
      "metadata": {
        "id": "l8lkmdbfofZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, Lambda"
      ],
      "metadata": {
        "id": "2bMRegWcoyLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Here’s what each import does:\n",
        "\n",
        "1. **`import tensorflow as tf`**:\n",
        "   - Imports the TensorFlow library, a powerful framework for building and training machine learning and deep learning models.\n",
        "\n",
        "2. **`from tensorflow.keras.models import Sequential`**:\n",
        "   - Imports the `Sequential` class, which is used to build models layer by layer. It’s ideal for models that have a simple, linear stack of layers.\n",
        "\n",
        "3. **`from tensorflow.keras.layers import Dense, Embedding, Lambda`**:\n",
        "   - **`Dense`**:\n",
        "     - A fully connected layer where each neuron receives input from all neurons in the previous layer.\n",
        "     - Used for output layers or intermediate layers in neural networks.\n",
        "   - **`Embedding`**:\n",
        "     - Converts integer-encoded words (like those in `X` and `Y`) into dense vectors of fixed size (`emb_size`), which serve as word embeddings.\n",
        "     - Useful for mapping word tokens to embeddings in natural language processing.\n",
        "   - **`Lambda`**:\n",
        "     - Allows you to wrap custom functions as Keras layers.\n",
        "     - Often used for simple operations (e.g., reshaping, arithmetic) that don’t require their own layer type.\n",
        "\n",
        "Together, these imports provide tools for creating and defining neural network models, especially ones used for embedding and NLP tasks."
      ],
      "metadata": {
        "id": "ajnPYLvwo5SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size),\n",
        "    Lambda(lambda x: tf.reduce_mean(x, axis=1)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "J-iNcMLApGN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code defines a neural network model using TensorFlow's Keras API. Here’s a breakdown of the layers:\n",
        "\n",
        "1. **`Sequential([ ... ])`**:\n",
        "   - Specifies that the model will be built in a linear stack of layers (from input to output).\n",
        "\n",
        "2. **`Embedding(input_dim=vocab_size, output_dim=emb_size, input_length=2*context_size)`**:\n",
        "   - This layer converts input tokens (e.g., the context words) into dense vectors (embeddings).\n",
        "   - **`input_dim=vocab_size`**: The size of the vocabulary (number of unique words).\n",
        "   - **`output_dim=emb_size`**: The size of the embedding vector for each word.\n",
        "   - **`input_length=2*context_size`**: The length of the input sequence (2 context words on each side of the target word).\n",
        "\n",
        "3. **`Lambda(lambda x: tf.reduce_mean(x, axis=1))`**:\n",
        "   - Applies a custom function to the output of the `Embedding` layer.\n",
        "   - **`tf.reduce_mean(x, axis=1)`** computes the mean of the embeddings along the context dimension (averaging the context word embeddings).\n",
        "\n",
        "4. **`Dense(256, activation='relu')`**:\n",
        "   - A fully connected layer with 256 neurons and the ReLU activation function, which introduces non-linearity.\n",
        "\n",
        "5. **`Dense(512, activation='relu')`**:\n",
        "   - Another fully connected layer, this time with 512 neurons and ReLU activation.\n",
        "\n",
        "6. **`Dense(vocab_size, activation='softmax')`**:\n",
        "   - The output layer, which has as many neurons as the size of the vocabulary (`vocab_size`).\n",
        "   - **`softmax` activation** is used to convert the output into probabilities, which can be interpreted as the likelihood of each word being the target word.\n",
        "\n",
        "This model is likely intended for tasks like word prediction or embedding learning, where the context (surrounding words) is used to predict the target word."
      ],
      "metadata": {
        "id": "0p5XnasLpJ3J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "M1j9si3XpSQp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This line of code compiles the neural network model:\n",
        "\n",
        "1. **`model.compile(...)`**:\n",
        "   - Prepares the model for training by specifying the loss function, optimizer, and evaluation metrics.\n",
        "\n",
        "2. **`loss='sparse_categorical_crossentropy'`**:\n",
        "   - Specifies the loss function used for multi-class classification tasks where each target word is represented by an integer index.\n",
        "   - **`sparse_categorical_crossentropy`** is suitable for tasks where the target values are integers (as opposed to one-hot encoded labels).\n",
        "\n",
        "3. **`optimizer='adam'`**:\n",
        "   - The optimizer used to minimize the loss function during training.\n",
        "   - **`Adam`** is an efficient and popular optimizer that adjusts learning rates dynamically during training.\n",
        "\n",
        "4. **`metrics=['accuracy']`**:\n",
        "   - Specifies that **`accuracy`** will be tracked as a performance metric during training.\n",
        "   - Accuracy measures how often the predicted word matches the actual target word.\n",
        "\n",
        "This setup configures the model to learn from data using the Adam optimizer and evaluate its performance based on accuracy."
      ],
      "metadata": {
        "id": "GGu8mpmlpXes"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(X, Y, epochs=80)\n"
      ],
      "metadata": {
        "id": "HU-4ljN7ph3T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The code:\n",
        "\n",
        "**`history = model.fit(X, Y, epochs=80)`**\n",
        "\n",
        "is used to train the model:\n",
        "\n",
        "1. **`model.fit(...)`**:\n",
        "   - Trains the model on the provided data (`X` and `Y`), adjusting its weights to minimize the loss function.\n",
        "\n",
        "2. **`X`**:\n",
        "   - The input data, in this case, the context words (converted into integer token sequences).\n",
        "\n",
        "3. **`Y`**:\n",
        "   - The target data, which are the integer tokens for the target words.\n",
        "\n",
        "4. **`epochs=80`**:\n",
        "   - Specifies the number of times the model will iterate over the entire training dataset (i.e., 80 passes through the data).\n",
        "   - Each epoch updates the model's weights to reduce the error (loss).\n",
        "\n",
        "5. **`history`**:\n",
        "   - Stores the training history, which includes information about the loss and accuracy after each epoch.\n",
        "   - This can be used later to analyze the model's performance during training (e.g., for plotting or debugging).\n",
        "\n",
        "This line trains the model for 80 epochs using the provided data."
      ],
      "metadata": {
        "id": "7U4jcA0Gplgo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "sns.lineplot(model.history.history)"
      ],
      "metadata": {
        "id": "hPD_Dboaps9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This line of code:\n",
        "\n",
        "**`sns.lineplot(model.history.history)`**\n",
        "\n",
        "is used to visualize the model's training progress using Seaborn:\n",
        "\n",
        "1. **`import seaborn as sns`**:\n",
        "   - Imports the Seaborn library, which is a powerful visualization tool built on top of Matplotlib.\n",
        "   - It makes it easy to create informative and attractive statistical plots.\n",
        "\n",
        "2. **`model.history.history`**:\n",
        "   - Refers to the training history of the model, which is stored in the `history` object after calling `model.fit()`.\n",
        "   - **`model.history.history`** is a dictionary that contains values like loss, accuracy, and other metrics recorded during training, for each epoch.\n",
        "\n",
        "3. **`sns.lineplot(...)`**:\n",
        "   - Creates a line plot using Seaborn.\n",
        "   - By passing `model.history.history` to `sns.lineplot()`, it plots the training metrics (like loss and accuracy) across epochs, showing how they change over time.\n",
        "\n",
        "This code helps visualize how the model's performance (e.g., loss and accuracy) evolves during training, making it easier to analyze the model's learning progress."
      ],
      "metadata": {
        "id": "UuxXDRSAp5cq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "embeddings = model.get_weights()[0]\n",
        "\n",
        "pca = PCA(n_components=2)\n",
        "reduced_embeddings = pca.fit_transform(embeddings)"
      ],
      "metadata": {
        "id": "-2xIcIXApy8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code performs **Principal Component Analysis (PCA)** to reduce the dimensionality of word embeddings for visualization:\n",
        "\n",
        "1. **`embeddings = model.get_weights()[0]`**:\n",
        "   - Retrieves the weights of the model, specifically the embeddings learned by the `Embedding` layer (the first set of weights).\n",
        "   - These embeddings are word vectors (dense representations) in a higher-dimensional space.\n",
        "\n",
        "2. **`pca = PCA(n_components=2)`**:\n",
        "   - Initializes a PCA object with the goal of reducing the embeddings to 2 dimensions (`n_components=2`).\n",
        "   - PCA is a technique that reduces the number of features while preserving as much variance as possible.\n",
        "\n",
        "3. **`reduced_embeddings = pca.fit_transform(embeddings)`**:\n",
        "   - Applies PCA to the `embeddings` to reduce their dimensionality from the original size (e.g., 10 or more dimensions) to 2 dimensions.\n",
        "   - **`fit_transform()`** fits the PCA model to the data and then transforms the embeddings into the reduced 2D space.\n",
        "\n",
        "This process allows the word embeddings to be visualized in 2D, making it easier to explore relationships between words (e.g., clustering similar words together)."
      ],
      "metadata": {
        "id": "0VqvwCB4p4rK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_sentenses = [\n",
        "    \"known as structured learning\",\n",
        "    \"transformers have applied to\",\n",
        "    \"where they produced results\",\n",
        "    \"cases surpassing expert performance\"\n",
        "]"
      ],
      "metadata": {
        "id": "vZzypwO-qDFH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "The variable **`test_sentences`** is a list of strings, where each string represents a sentence. Here's a breakdown:\n",
        "\n",
        "1. **`test_sentences = [...]`**:\n",
        "   - This defines a list called `test_sentences` containing 4 example sentences as elements.\n",
        "\n",
        "Each sentence in this list is a short fragment of text. These sentences could be used for testing or further processing, such as tokenization, embedding generation, or model inference, depending on the task at hand.\n",
        "\n",
        "For example:\n",
        "- \"known as structured learning\"\n",
        "- \"transformers have applied to\"\n",
        "- \"where they produced results\"\n",
        "- \"cases surpassing expert performance\"\n",
        "\n",
        "These sentences might be fed into a trained model for tasks like text classification, prediction, or embedding visualization."
      ],
      "metadata": {
        "id": "W2MLBptMqGDw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sent in test_sentenses:\n",
        "    test_words = sent.split(\" \")\n",
        "#     print(test_words)\n",
        "    x_test =[]\n",
        "    for i in test_words:\n",
        "        x_test.append(word_to_index.get(i))\n",
        "    x_test = np.array([x_test])\n",
        "#     print(x_test)\n",
        "\n",
        "    pred = model.predict(x_test)\n",
        "    pred = np.argmax(pred[0])\n",
        "    print(\"pred \", test_words, \"\\n=\", index_to_word.get(pred),\"\\n\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "eMXtjG5hqQJt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "This code is used to predict the target word for each sentence in `test_sentences` using the trained model. Here's a breakdown:\n",
        "\n",
        "1. **`for sent in test_sentences:`**:\n",
        "   - Iterates over each sentence in the `test_sentences` list.\n",
        "\n",
        "2. **`test_words = sent.split(\" \")`**:\n",
        "   - Splits the sentence into individual words (tokens) using spaces as separators.\n",
        "\n",
        "3. **`x_test = []`**:\n",
        "   - Initializes an empty list `x_test` to store the tokenized word indices (based on `word_to_index`).\n",
        "\n",
        "4. **`for i in test_words:`**:\n",
        "   - Loops through each word in `test_words`.\n",
        "\n",
        "5. **`x_test.append(word_to_index.get(i))`**:\n",
        "   - Converts each word to its corresponding index (token) using the `word_to_index` dictionary and appends it to `x_test`.\n",
        "\n",
        "6. **`x_test = np.array([x_test])`**:\n",
        "   - Converts `x_test` to a NumPy array and reshapes it to match the input shape expected by the model.\n",
        "\n",
        "7. **`pred = model.predict(x_test)`**:\n",
        "   - Uses the trained model to predict the next word (target) given the context (`x_test`).\n",
        "\n",
        "8. **`pred = np.argmax(pred[0])`**:\n",
        "   - `model.predict()` returns a probability distribution across all vocabulary words.\n",
        "   - `np.argmax(pred[0])` retrieves the index of the highest probability word (the predicted target word).\n",
        "\n",
        "9. **`print(\"pred \", test_words, \"\\n=\", index_to_word.get(pred),\"\\n\\n\")`**:\n",
        "   - Prints the original words in the sentence (`test_words`), followed by the predicted target word (using `index_to_word.get(pred)` to map the index back to the word).\n",
        "\n",
        "In summary, this code processes each test sentence, tokenizes it, predicts the target word using the trained model, and prints the results."
      ],
      "metadata": {
        "id": "UQ2t3jzgqTjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zOK0eVaTqci_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}