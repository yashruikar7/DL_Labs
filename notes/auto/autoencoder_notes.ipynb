{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPyXLZpohFC"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "This line imports the StandardScaler class from the sklearn.preprocessing module. StandardScaler is a tool in scikit-learn used to standardize features by removing the mean and scaling to unit variance. This preprocessing step ensures that each feature contributes equally to the model, making algorithms less sensitive to feature scaling.\n",
        "This line initializes an instance of the StandardScaler class. After creating an instance, StandardScaler can be used to fit and transform data based on the mean and standard deviation of each feature in the dataset.\n",
        "Here, fit_transform is applied to df (assuming df is a DataFrame or a 2D array). fit_transform first computes the mean and standard deviation for each feature in df, then scales each feature by subtracting the mean and dividing by the standard deviation. This returns a standardized version of df, where each feature will have a mean of 0 and a standard deviation of 1. The result is assigned back to df.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.models.Sequential([\n",
        "    layers.Input(shape=(x_train.shape[1],)),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(8, activation='relu')\n",
        "])\n",
        "\n",
        "decoder = tf.keras.models.Sequential([\n",
        "    layers.Input(shape=(8,)),\n",
        "    layers.Dense(16, activation='relu'),\n",
        "    layers.Dense(32, activation='relu'),\n",
        "    layers.Dense(x_train.shape[1], activation='linear')  # Use linear activation for reconstruction\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "layers.Input(shape=(x_train.shape[1],)): Specifies the input shape for the encoder, matching the number of features in the training data x_train. x_train.shape[1] gives the number of features (or columns).\n",
        "layers.Dense(32, activation='relu'): The first hidden layer with 32 units, using the ReLU activation function. ReLU (Rectified Linear Unit) is a popular activation function that helps models learn complex patterns by introducing nonlinearity.\n",
        "layers.Dense(16, activation='relu'): The second hidden layer with 16 units, also using ReLU.\n",
        "layers.\n",
        "\n",
        "layers.Input(shape=(8,)): Specifies the input shape for the decoder, matching the output shape of the encoder's final layer (8 units).\n",
        "layers.Dense(16, activation='relu'): First hidden layer of the decoder with 16 units, using ReLU activation.\n",
        "layers.Dense(32, activation='relu'): Second hidden layer of the decoder with 32 units, also using ReLU.\n",
        "layers.Dense(x_train.shape[1], activation='linear'): Output layer of the decoder, with the same number of units as the original input dimension (x_train.shape[1]). It uses a linear activation function, which is suitable for reconstructing continuous-valued data.\n",
        "\n"
      ],
      "metadata": {
        "id": "s0gQ_3dHpRn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss ='mean_squared_error')\n",
        "\n",
        "The optimizer controls how the model updates its weights based on the loss function's output. Here, 'adam' (short for Adaptive Moment Estimation) is used:\n",
        "\n",
        "Adam combines two other optimization techniques: momentum and RMSProp.\n",
        "Momentum helps smooth out updates by considering past gradients, making it easier for the model to avoid local minima and find better solutions.\n",
        "RMSProp adjusts the learning rate of each parameter based on its history of gradients, allowing more nuanced updates.\n",
        "Adam adjusts the learning rate throughout training based on the gradient's momentum and scale, making it effective and fast for a wide range of tasks.\n",
        "Overall, Adam is popular because it works well across different types of models and datasets without much tuning of the learning rate and other hyperparameters.\n",
        "\n",
        "Loss Function: 'mean_squared_error'\n",
        "The loss function quantifies the difference between the model's predictions and the actual target values. In this case, mean_squared_error (MSE) is used, which is commonly used for regression tasks or, as in your case, for reconstruction in autoencoders.\n",
        "\n",
        "Mean Squared Error calculates the average of the squared differences between predicted values and actual values.\n",
        "For each prediction, it finds the error by subtracting the true value from the predicted value.\n",
        "It squares this error (to ensure it’s positive and penalize larger errors more heavily).\n",
        "Then it averages these squared errors across all data points.\n",
        "Using MSE encourages the model to make predictions that are close to the actual values, aiming to reduce large errors effectively."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "id": "LhD8iffiqlH1",
        "outputId": "44596a0a-27c3-42f0-fac1-1738a38eb931"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated string literal (detected at line 3) (<ipython-input-1-825419f217da>, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-825419f217da>\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    The optimizer controls how the model updates its weights based on the loss function's output. Here, 'adam' (short for Adaptive Moment Estimation) is used:\u001b[0m\n\u001b[0m                                                                                                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated string literal (detected at line 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'],label='val_loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "history.history['loss'] retrieves the training loss for each epoch, and plt.plot(...) plots it.\n",
        "history.history['val_loss'] retrieves the validation loss for each epoch, and it’s also plotted.\n",
        "Adding label='loss' and label='val_loss' names each line in the legend.\n",
        "plt.legend() displays the labels (\"loss\" and \"val_loss\") on the plot.\n",
        "plt.show() renders the plot, showing how both the training loss and validation loss change across epochs.\n"
      ],
      "metadata": {
        "id": "V72j5OnRq038"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(x_test)\n",
        "mse = np.mean(np.power(x_test - predictions, 2), axis=1)\n",
        "\n",
        "\n",
        "This code snippet calculates the Mean Squared Error (MSE) between the actual test data (x_test) and the model's reconstructed predictions (predictions). It's commonly used in evaluating the performance of an autoencoder to see how closely the model can reconstruct the input data.\n",
        "This line generates predictions for x_test using the model. Since this is likely an autoencoder model, it attempts to reconstruct each input in x_test, making predictions resemble x_test as closely as possible.\n",
        "This line calculates the Mean Squared Error (MSE) for each instance in x_test:\n",
        "x_test - predictions computes the element-wise difference between the original input data and its reconstructed prediction.\n",
        "np.power(..., 2) squares each of these differences, emphasizing larger errors more heavily.\n",
        "np.mean(..., axis=1) takes the mean of these squared differences across each feature for each instance, resulting in a single MSE value per instance.\n"
      ],
      "metadata": {
        "id": "B6z8tTmErqO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.percentile(mse, 95) calculates the 95th percentile of the mse values. This means that threshold will be set to a value such that 95% of the MSE values are below it and 5% are above it.\n",
        "This threshold can be used as a cutoff for anomaly detection: any data point with an MSE above this threshold can be flagged as an anomaly (or an \"outlier\"), since it has a reconstruction error that is unusually high compared to most other points.\n",
        "Why Use the 95th Percentile?\n",
        "Setting the threshold at the 95th percentile captures the majority (95%) of data as normal and flags the top 5% with the highest reconstruction errors as potential anomalies.\n",
        "You can adjust the percentile based on the specific needs of the application or based on experimentation to achieve the desired level of sensitivity to anomalies.\n",
        "\n",
        "threshold = np.percentile(mse, 95)  # Adjust the percentile as needed\n",
        "threshold"
      ],
      "metadata": {
        "id": "tGVKPTRWsCjn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "anomalies = mse > threshold\n",
        "\n",
        "mse > threshold creates a boolean array where each element is True if the corresponding MSE value exceeds the threshold, and False otherwise.\n",
        "If mse[i] > threshold, the model considers the sample i to have a high reconstruction error, suggesting it could be an anomaly (or outlier) compared to typical data.\n",
        "The result, anomalies, is an array of boolean values (True or False), where each True indicates that the sample is classified as an anomaly."
      ],
      "metadata": {
        "id": "QI2DCuX1sVun"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_anomalies = np.sum(anomalies)\n",
        "print(f\"Number of Anomalies: {num_anomalies}\")\n",
        "\n",
        "np.sum(anomalies) counts the number of True values in the anomalies array. Since each True represents a detected anomaly, np.sum(anomalies) gives the total count of anomalies.\n",
        "This count is stored in num_anomalies.\n",
        "This line prints out the total number of anomalies in a formatted string."
      ],
      "metadata": {
        "id": "L9tbzR-5tJXu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(mse, marker='o', linestyle='', markersize=3, label='MSE')\n",
        "plt.axhline(threshold, color='r', linestyle='--', label='Anomaly Threshold')\n",
        "plt.xlabel('Sample Index')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('Anomaly Detection Results')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "This line plots each sample's MSE as a scatter plot (marker='o' with linestyle='' removes lines between points) where each point represents the MSE of a sample.\n",
        "markersize=3 makes each marker (dot) small for clearer visualization.\n",
        "label='MSE' names this series as \"MSE\" in the legend."
      ],
      "metadata": {
        "id": "WzT1ElEKtd1o"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}