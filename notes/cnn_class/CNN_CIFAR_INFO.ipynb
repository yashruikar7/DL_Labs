{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ded55a-b7a0-4f52-825f-226003dd1a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as  np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import BatchNormalization, Conv2D, Dense, Dropout, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f60b6022-9643-4d26-b5f8-564a2ce2952b",
   "metadata": {},
   "outputs": [],
   "source": [
    "The code you provided imports several essential libraries and modules typically used in machine learning, data analysis, and deep learning projects. Here's a breakdown of each import:\n",
    "\n",
    "1. **Pandas** (`import pandas as pd`):\n",
    "   - **Purpose**: This library is mainly used for data manipulation and analysis. It allows you to load, clean, and transform datasets, typically in the form of data frames (tables).\n",
    "   - **Common Usage**: Handling structured data, reading and writing files like CSV, Excel, etc.\n",
    "\n",
    "2. **NumPy** (`import numpy as np`):\n",
    "   - **Purpose**: A powerful library for numerical computing. It provides support for arrays, matrices, and mathematical functions to perform operations on them.\n",
    "   - **Common Usage**: Numerical calculations, array manipulations, linear algebra.\n",
    "\n",
    "3. **Matplotlib** (`import matplotlib.pyplot as plt`):\n",
    "   - **Purpose**: A plotting library for creating static, interactive, and animated visualizations in Python.\n",
    "   - **Common Usage**: Data visualization, plotting graphs like histograms, line charts, scatter plots, etc.\n",
    "\n",
    "4. **Seaborn** (`import seaborn as sns`):\n",
    "   - **Purpose**: A data visualization library based on Matplotlib that makes it easier to create more attractive and informative statistical graphics.\n",
    "   - **Common Usage**: Creating statistical plots such as heatmaps, pair plots, and box plots.\n",
    "\n",
    "5. **TensorFlow** (`import tensorflow as tf`):\n",
    "   - **Purpose**: A deep learning framework developed by Google. It is used for building and training machine learning models, particularly neural networks.\n",
    "   - **Common Usage**: Neural network creation, training, and deployment for tasks like image recognition, NLP, etc.\n",
    "\n",
    "6. **Keras Layers** (e.g., `BatchNormalization`, `Conv2D`, `Dense`, `Dropout`, `MaxPooling2D`, `Flatten`):\n",
    "   - **Purpose**: These are specific building blocks (layers) used to construct deep learning models in Keras (which is part of TensorFlow).\n",
    "     - **BatchNormalization**: Normalizes the output of a previous activation layer to improve the training of deep neural networks.\n",
    "     - **Conv2D**: Convolutional layer for 2D images, essential in Convolutional Neural Networks (CNNs) for tasks like image classification.\n",
    "     - **Dense**: Fully connected layer, where each neuron is connected to every neuron in the previous layer.\n",
    "     - **Dropout**: Regularization technique to prevent overfitting by randomly dropping neurons during training.\n",
    "     - **MaxPooling2D**: Reduces the dimensionality of images by taking the maximum value from each feature map region.\n",
    "     - **Flatten**: Converts a multi-dimensional array into a one-dimensional array, typically used before feeding data into a fully connected layer.\n",
    "   \n",
    "7. **Keras Model and Sequential**:\n",
    "   - **Purpose**: These are classes for defining the structure of a neural network.\n",
    "     - **Model**: A more flexible way to define complex models with multiple inputs and outputs.\n",
    "     - **Sequential**: A simpler, linear stack of layers for building neural networks where each layer has exactly one input and one output.\n",
    "\n",
    "8. **SGD (Stochastic Gradient Descent)** (`from tensorflow.keras.optimizers import SGD`):\n",
    "   - **Purpose**: A popular optimization algorithm used to minimize the loss function during training. It updates model parameters in the direction that reduces the error.\n",
    "\n",
    "In summary, this code sets up libraries for:\n",
    "- **Data processing** (Pandas, NumPy),\n",
    "- **Data visualization** (Matplotlib, Seaborn),\n",
    "- **Deep learning** (TensorFlow, Keras layers), and\n",
    "- **Model optimization** (SGD optimizer).\n",
    "\n",
    "This setup is commonly used for creating and training deep learning models, especially Convolutional Neural Networks (CNNs), for tasks like image classification or object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29b66085-1c5b-471a-8df0-c3768bd77b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop(['label'], axis = 1).values\n",
    "y_train = train['label'].values\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c10a198-dba1-4e0b-8eca-f3ca552dcb4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "This assumes that train is a pandas DataFrame containing the training data.\n",
    "\n",
    "    x_train = train.drop(['label'], axis = 1).values:\n",
    "        This line removes the column named 'label' from the train DataFrame and stores the remaining features (input variables) in the x_train variable.\n",
    "        .drop(['label'], axis = 1): Drops the 'label' column (axis 1 indicates columns).\n",
    "        .values: Converts the DataFrame to a NumPy array.\n",
    "        x_train will thus contain the features (input data) of the training set, excluding the target labels.\n",
    "\n",
    "    y_train = train['label'].values:\n",
    "        This line extracts the 'label' column from the train DataFrame and stores it as a NumPy array in y_train.\n",
    "        y_train will contain the target labels (outputs) of the training set.\n",
    "\n",
    "    x_train.shape:\n",
    "        This returns the shape of the x_train array, which will give you the number of samples (rows) and the number of features (columns).\n",
    "        The result of x_train.shape is typically a tuple like (num_samples, num_features)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3dad254-de6e-4c97-95e5-f5d6e54a9da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape((50000, 32, 32, 3)).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b451d4-c032-43d5-9a5b-acddd5de85bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.reshape((50000, 32, 32, 3)):\n",
    "\n",
    "    .reshape(): This function changes the shape of the x_train array to the new dimensions (50000, 32, 32, 3).\n",
    "        50000: This is the number of images (or samples) in the dataset.\n",
    "        32x32: This is the height and width of each image. In this case, each image is a 32x32 pixel image.\n",
    "        3: This indicates the number of color channels (RGB). Each image has three channels: Red, Green, and Blue.\n",
    "    Essentially, this reshapes x_train into a 4D array where each image has the shape (32, 32, 3) and there are 50,000 such images.\n",
    "\n",
    ".astype('float32'):\n",
    "\n",
    "    This converts the data type of x_train to float32. This is a common practice in deep learning to ensure the model can perform computations efficiently. Most deep learning models expect inputs as float32 values (32-bit floating point numbers).\n",
    "\n",
    "/255:\n",
    "\n",
    "    This normalizes the pixel values of the images. Image pixel values typically range from 0 to 255 (for 8-bit images).\n",
    "    Dividing by 255 scales the pixel values to the range [0, 1], which is more suitable for neural networks, as it helps the model learn more efficiently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febbfb65-6846-4eb1-a9cd-43ffff19401a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), input_shape = (32, 32, 3), activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64, (3,3),activation = 'relu'))\n",
    "model.add(MaxPooling2D((2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation = 'relu'))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(10, activation = 'softmax'))\n",
    "\n",
    "sgd = SGD(learning_rate = 0.1)\n",
    "model.compile(optimizer= sgd, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a3ce0c-6dd4-4f90-ab09-68fb52c006da",
   "metadata": {},
   "outputs": [],
   "source": [
    "The code you provided defines a Convolutional Neural Network (CNN) model using the **Keras Sequential API**, and then compiles the model for training. Hereâ€™s a breakdown of each part:\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "1. **`model = Sequential()`**:\n",
    "   - This initializes a Sequential model, which is a linear stack of layers, meaning that each layer has exactly one input and one output.\n",
    "\n",
    "2. **`model.add(Conv2D(32, (3,3), input_shape = (32, 32, 3), activation = 'relu'))`**:\n",
    "   - **Conv2D**: This is a 2D convolutional layer, commonly used for image processing tasks.\n",
    "     - **32**: The number of filters (or kernels) in this layer. It will learn 32 different features (e.g., edges, textures).\n",
    "     - **(3, 3)**: The size of the convolutional filter (3x3 pixels).\n",
    "     - **input_shape = (32, 32, 3)**: The shape of the input data (32x32 RGB images).\n",
    "     - **activation = 'relu'**: The ReLU (Rectified Linear Unit) activation function, which is commonly used in CNNs to introduce non-linearity.\n",
    "\n",
    "3. **`model.add(BatchNormalization())`**:\n",
    "   - **BatchNormalization**: This normalizes the activations of the previous layer to help stabilize the learning process, improve convergence, and reduce overfitting.\n",
    "\n",
    "4. **`model.add(Conv2D(64, (3,3), activation = 'relu'))`**:\n",
    "   - Another 2D convolutional layer with 64 filters. This layer will learn more complex features from the data after the initial convolution.\n",
    "\n",
    "5. **`model.add(MaxPooling2D((2,2)))`**:\n",
    "   - **MaxPooling2D**: This layer reduces the spatial dimensions of the output from the previous convolutional layer (downsampling). It takes the maximum value from a 2x2 region, effectively reducing the image size by half.\n",
    "\n",
    "6. **`model.add(Flatten())`**:\n",
    "   - **Flatten**: This layer converts the 2D matrix (output of the convolution and pooling layers) into a 1D vector. This is necessary before passing the data to fully connected (Dense) layers.\n",
    "\n",
    "7. **`model.add(Dense(64, activation = 'relu'))`**:\n",
    "   - **Dense**: A fully connected layer with 64 neurons.\n",
    "   - **activation = 'relu'**: Using the ReLU activation function again for introducing non-linearity.\n",
    "\n",
    "8. **`model.add(Dropout(0.2))`**:\n",
    "   - **Dropout**: This is a regularization technique to prevent overfitting by randomly setting 20% of the neurons to zero during training.\n",
    "\n",
    "9. **`model.add(Dense(10, activation = 'softmax'))`**:\n",
    "   - Another **Dense** layer, but this time with 10 neurons (because the model is likely for classification with 10 possible output classes, such as digits 0-9 in MNIST).\n",
    "   - **activation = 'softmax'**: The softmax activation function is used in the output layer for multi-class classification. It will output probabilities that sum to 1, indicating the likelihood of each class.\n",
    "\n",
    "### Model Compilation\n",
    "\n",
    "10. **`sgd = SGD(learning_rate = 0.1)`**:\n",
    "    - **SGD**: This is the Stochastic Gradient Descent optimizer.\n",
    "    - **learning_rate = 0.1**: The learning rate is set to 0.1, which controls the size of the steps the model takes while minimizing the loss function.\n",
    "\n",
    "11. **`model.compile(optimizer= sgd, loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])`**:\n",
    "    - **optimizer = sgd**: Specifies the optimization algorithm (SGD in this case).\n",
    "    - **loss = 'sparse_categorical_crossentropy'**: This loss function is used for multi-class classification when the labels are integers (not one-hot encoded). It calculates the cross-entropy loss between the predicted probabilities and the true class labels.\n",
    "    - **metrics = ['accuracy']**: Specifies that accuracy should be tracked during training.\n",
    "\n",
    "### Model Summary\n",
    "\n",
    "12. **`model.summary()`**:\n",
    "    - This function will print a summary of the model architecture, including the number of parameters in each layer, the total number of parameters, and the shapes of the output tensors at each stage.\n",
    "\n",
    "### In summary:\n",
    "- This model is a Convolutional Neural Network (CNN) with:\n",
    "  - 2 convolutional layers with ReLU activations.\n",
    "  - Batch normalization to stabilize training.\n",
    "  - MaxPooling to reduce the image size.\n",
    "  - Fully connected layers with Dropout for regularization.\n",
    "  - A final Softmax output layer for multi-class classification (with 10 classes).\n",
    "  \n",
    "This CNN is designed for tasks such as image classification and should perform well on datasets like MNIST, CIFAR-10, or other image datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26f364b0-392f-451a-ab59-f39b48088976",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(x_train, y_train, validation_split=0.2, epochs = 11, batch_size = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e021a6a-c648-4f0e-ac0d-cd192611a0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    x_train:\n",
    "        This is the input data (features) for training, which should be a NumPy array of shape (num_samples, 32, 32, 3). It represents the images in your dataset.\n",
    "\n",
    "    y_train:\n",
    "        This is the target data (labels) for training, which should be a NumPy array containing the correct class for each image in x_train. This is the output the model is trying to predict.\n",
    "\n",
    "    validation_split=0.2:\n",
    "        Validation Split: The model will use 20% of the training data for validation and the remaining 80% for training. This helps track the model's performance on unseen data during training, which is useful for monitoring overfitting.\n",
    "        The model will split the data automatically during training: 80% for training and 20% for validation.\n",
    "\n",
    "    epochs=11:\n",
    "        Epochs: This specifies the number of times the entire training dataset will pass through the model. In this case, the model will be trained for 11 epochs. More epochs generally mean better training, but can also lead to overfitting if the model learns too much noise from the data.\n",
    "\n",
    "    batch_size=64:\n",
    "        Batch Size: This is the number of samples used in each gradient update during training. The model will process 64 samples at a time before updating the modelâ€™s weights. A smaller batch size can lead to noisier updates, while a larger batch size can lead to faster, more stable updates.\n",
    "\n",
    "Output of model.fit()\n",
    "\n",
    "    The method returns an object (r in this case), which is a History object.\n",
    "    The History object contains the training and validation loss/accuracy for each epoch. This can be accessed via r.history, where:\n",
    "        r.history['loss']: The training loss at each epoch.\n",
    "        r.history['accuracy']: The training accuracy at each epoch.\n",
    "        r.history['val_loss']: The validation loss at each epoch.\n",
    "        r.history['val_accuracy']: The validation accuracy at each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1d5565-e703-41e8-883c-db7d78d47c9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(['label'], axis = 1).values\n",
    "y_test = test['label'].values\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4644e6c9-e450-439a-9f8d-753f637b4447",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(['label'], axis = 1).values:\n",
    "\n",
    "    test.drop(['label'], axis = 1): This drops the 'label' column from the test DataFrame, which contains the target values. The remaining columns are the features (input data).\n",
    "    .values: This converts the DataFrame into a NumPy array, so x_test is now a NumPy array of the feature data.\n",
    "    x_test will contain the input features (images) for the test set.\n",
    "\n",
    "y_test = test['label'].values:\n",
    "\n",
    "    test['label']: This extracts the 'label' column from the test DataFrame, which contains the true class labels for the test set.\n",
    "    .values: This converts the labels into a NumPy array.\n",
    "    y_test will contain the target labels for the test set.\n",
    "\n",
    "x_test.shape:\n",
    "\n",
    "    This returns the shape of the x_test array, which tells you the dimensions of the test dataset.\n",
    "    The result will be in the form (num_samples, num_features). If the data consists of images, the number of features is likely to be the flattened number of pixels per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce60af69-507c-4a1e-a3be-4981853957c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.reshape((10000, 32, 32, 3)).astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180de901-eada-41ad-823f-0be039c0bfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.reshape((10000, 32, 32, 3)):\n",
    "\n",
    "    .reshape(): This function changes the shape of the x_test array to the new dimensions (10000, 32, 32, 3).\n",
    "        10000: The number of test images (or samples).\n",
    "        32x32: The height and width of each image, indicating that the images are 32x32 pixels.\n",
    "        3: The number of color channels, with 3 representing the RGB color channels.\n",
    "    This reshapes x_test into a 4D array where each image has the shape (32, 32, 3) and there are 10,000 such images.\n",
    "\n",
    ".astype('float32'):\n",
    "\n",
    "    This converts the x_test data type to float32. This is a common practice to make computations more efficient and to match the expected input type for TensorFlow/Keras models.\n",
    "    float32 is the standard data type for training and evaluating models in deep learning.\n",
    "\n",
    "/255:\n",
    "\n",
    "    This normalizes the pixel values to the range [0, 1]. Image pixel values usually range from 0 to 255 (for 8-bit color channels).\n",
    "    Dividing by 255 scales the pixel values so they are between 0 and 1, which is more suitable for neural network models. This normalization improves model convergence and training efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c2f3af4-8dec-4e89-90f1-ee204e562ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e4df2a-f6d6-4728-ad7d-389eb8d7e5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "    model.predict(x_test):\n",
    "        model.predict(): This method is used to make predictions using the trained model. It takes the test data (x_test) as input and outputs the model's predicted labels (or probabilities, depending on the output layer).\n",
    "        x_test: This is the test data that has been preprocessed (reshaped, normalized, and converted to float32), representing the input images for which you want the model to make predictions.\n",
    "    What model.predict() returns:\n",
    "        Since the modelâ€™s output layer uses a softmax activation function (as defined in the architecture), it will output a probability distribution for each class for each image.\n",
    "        For each test image, the output will be a vector of probabilities, with each element corresponding to the predicted probability of each class (in this case, there are 10 classes because the final layer has 10 neurons).\n",
    "        For example, if you have 10,000 test images, predictions will be an array of shape (10000, 10), where each row contains the predicted probability distribution over the 10 classes for each image.\n",
    "\n",
    "What you can do with the predictions:\n",
    "\n",
    "    Class Predictions: If you want to get the predicted class (the class with the highest probability), you can use:\n",
    "\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "\n",
    "    np.argmax(predictions, axis=1): This will return the index of the class with the highest probability for each image, essentially giving you the modelâ€™s predicted class label for each image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b841a871-4b4b-4704-9d21-a54f0aba6153",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)\n",
    "print(f'Test Loss: {test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fed0ff5-affa-48e8-8265-c312f9093922",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, y_test):\n",
    "\n",
    "    This method evaluates the trained model on the test dataset (x_test and y_test).\n",
    "    It computes the loss and accuracy of the model on the test data.\n",
    "        Loss: A measure of how well the model's predictions match the true labels. Lower loss is better.\n",
    "        Accuracy: The proportion of correct predictions. Higher accuracy is better.\n",
    "\n",
    "test_loss and test_accuracy:\n",
    "\n",
    "    These variables store the calculated loss and accuracy from the evaluation process.\n",
    "\n",
    "print(f'Test Loss: {test_loss:.4f}'):\n",
    "\n",
    "    Prints the test loss, formatted to 4 decimal places.\n",
    "\n",
    "print(f'Test Accuracy: {test_accuracy:.4f}'):\n",
    "\n",
    "    Prints the test accuracy, formatted to 4 decimal places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3055cc2-2767-4cf7-962c-a4e8d796edb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5726703-8c6b-480e-b233-1200d3e4c666",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names is a list containing the names of the classes in a classification task, where each class corresponds to a label for a category (e.g., types of objects in images).\n",
    "In this case, it represents the CIFAR-10 dataset, which consists of 10 different classes of images:\n",
    "\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8448abb8-2bba-4dc0-a24b-c7d74347c17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['accuracy'], label = 'accuracy', color = 'green'):\n",
    "\n",
    "    This plots the training accuracy over each epoch. The r.history['accuracy'] contains the accuracy values recorded during the training process for each epoch.\n",
    "    The line is labeled as 'accuracy' and is colored green.\n",
    "\n",
    "plt.plot(r.history['val_accuracy'], label = 'val_accuracy', color = 'red'):\n",
    "\n",
    "    This plots the validation accuracy over each epoch. The r.history['val_accuracy'] contains the accuracy values for the validation data recorded during training.\n",
    "    The line is labeled as 'val_accuracy' and is colored red.\n",
    "\n",
    "plt.legend():\n",
    "\n",
    "    This displays a legend to distinguish between the two plotted lines (training accuracy and validation accuracy) using the labels 'accuracy' and 'val_accuracy'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0424bd-dc3a-4ca8-b465-3d5cfc771b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss', color = 'red')\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss', color = 'blue')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e26c4a-3034-4d7a-ab8a-986bfe6737a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss', color = 'red'):\n",
    "\n",
    "    This plots the training loss over each epoch. The r.history['loss'] contains the loss values recorded during the training process for each epoch.\n",
    "    The line is labeled as 'loss' and is colored red.\n",
    "\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss', color = 'blue'):\n",
    "\n",
    "    This plots the validation loss over each epoch. The r.history['val_loss'] contains the loss values for the validation data recorded during training.\n",
    "    The line is labeled as 'val_loss' and is colored blue.\n",
    "\n",
    "plt.legend():\n",
    "\n",
    "    This displays a legend to differentiate between the two plotted lines (training loss and validation loss) using the labels 'loss' and 'val_loss'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8701c20-90ba-432c-87bc-a0ddfbcd0c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=x_train[5]\n",
    "plt.imshow(np.squeeze(image),cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb239992-debb-46a3-938f-4995931b6532",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = x_train[5]:\n",
    "\n",
    "    This selects the 6th image (since indexing starts from 0) from the x_train array. The x_train array contains your training images, each of size 32x32 pixels with 3 color channels (RGB), so image will be of shape (32, 32, 3).\n",
    "\n",
    "np.squeeze(image):\n",
    "\n",
    "    This function removes any singleton dimensions (i.e., dimensions of size 1). For a typical RGB image with shape (32, 32, 3), this operation is not strictly necessary. It's generally used when there's an extra dimension (e.g., (1, 32, 32, 3)), but it ensures the image is properly formatted for display.\n",
    "\n",
    "plt.imshow(np.squeeze(image), cmap='gray'):\n",
    "\n",
    "    plt.imshow(): This is the function from Matplotlib used to display images.\n",
    "    cmap='gray': This applies the grayscale colormap, which converts the image to grayscale for display. Since the image is RGB, using cmap='gray' is not ideal because it will remove the color information, converting the image into shades of gray based on pixel intensity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962617cc-4d43-4b50-8b0f-4783d380ed99",
   "metadata": {},
   "outputs": [],
   "source": [
    "image=image.reshape(1, image.shape[0], image.shape[1], image.shape[2])\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81203cbe-af73-4587-90db-7ef99980e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "image.shape:\n",
    "\n",
    "    Before the reshaping, the shape of image is (32, 32, 3), which represents a single image of size 32x32 pixels with 3 color channels (RGB).\n",
    "\n",
    ".reshape(1, image.shape[0], image.shape[1], image.shape[2]):\n",
    "\n",
    "    reshape() changes the dimensions of the image. The new shape specified here is (1, 32, 32, 3).\n",
    "        1: This is the batch size. Neural networks typically process images in batches, so this represents a batch with one image.\n",
    "        32: The height of the image (32 pixels).\n",
    "        32: The width of the image (32 pixels).\n",
    "        3: The number of color channels (RGB).\n",
    "\n",
    "Result:\n",
    "\n",
    "    After reshaping, image will have the shape (1, 32, 32, 3), representing a batch of one image.\n",
    "    This reshaped format is necessary when feeding data into a neural network, as models expect the input in batches, even if the batch contains just one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2d6936-7665-4397-9ce8-839d123dd911",
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_model = model.predict([image])\n",
    "print(\"Pedicted class: {}\".format(np.argmax(predict_model)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c594b0c-c3f3-4539-962d-00f57e1ea983",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict([image]):\n",
    "\n",
    "    model.predict(): This method uses the trained model to make predictions on the given input.\n",
    "    [image]: The input is a batch of images, and since you've reshaped image to (1, 32, 32, 3), it is passed as a batch containing one image. The model will output a prediction for this single image.\n",
    "\n",
    "predict_model:\n",
    "\n",
    "    predict_model will be a 2D array (of shape (1, 10)), where the first dimension is the batch size (1 image), and the second dimension corresponds to the 10 classes. The values in this array are the probabilities for each class.\n",
    "    For example, predict_model[0] could look like [0.1, 0.3, 0.2, 0.05, 0.05, 0.1, 0.15, 0.05, 0.01, 0.04], which represents the predicted probabilities for each of the 10 classes.\n",
    "\n",
    "np.argmax(predict_model):\n",
    "\n",
    "    np.argmax(predict_model): This function returns the index of the maximum value in predict_model[0], which corresponds to the predicted class. Since the model uses softmax, the output is a probability distribution, and the class with the highest probability is chosen as the predicted class.\n",
    "\n",
    "print(\"Predicted class: {}\".format(np.argmax(predict_model))):\n",
    "\n",
    "    This prints the predicted class index (from 0 to 9) corresponding to the highest probability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
