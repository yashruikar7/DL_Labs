{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b118380-bbba-42eb-b17e-b60cab0bf5cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, MaxPooling2D, Conv2D\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efc4b87c-070d-4ff0-857a-61d564133e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code imports several essential libraries and tools for data analysis, machine learning, and deep learning. Here's a breakdown of what each import does:\n",
    "\n",
    "1. **`import pandas as pd`**: \n",
    "   - This imports the `pandas` library, which is used for data manipulation and analysis, particularly for handling structured data (like tables). It is commonly used to work with data in DataFrame format.\n",
    "\n",
    "2. **`import numpy as np`**: \n",
    "   - This imports the `numpy` library, which provides support for large, multi-dimensional arrays and matrices, along with a collection of mathematical functions to operate on these arrays. It’s a key library for numerical computations in Python.\n",
    "\n",
    "3. **`import matplotlib.pyplot as plt`**: \n",
    "   - This imports the `matplotlib` library, specifically its `pyplot` module, which is used for creating static, animated, and interactive visualizations like graphs, charts, and plots.\n",
    "\n",
    "4. **`import tensorflow as tf`**: \n",
    "   - This imports the `tensorflow` library, which is a popular open-source deep learning framework used for building and training machine learning models, particularly neural networks.\n",
    "\n",
    "5. **`import seaborn as sns`**: \n",
    "   - This imports `seaborn`, a data visualization library based on `matplotlib`. It provides a high-level interface for drawing attractive statistical graphics.\n",
    "\n",
    "6. **`from tensorflow.keras.layers import BatchNormalization, Dense, Dropout, MaxPooling2D, Conv2D`**:\n",
    "   - These imports come from the `keras` module within `tensorflow`. They are different types of layers used in building neural networks:\n",
    "     - `BatchNormalization`: This layer normalizes the input to a layer to improve training speed and stability.\n",
    "     - `Dense`: A fully connected layer, where each input is connected to each output.\n",
    "     - `Dropout`: A regularization technique that randomly drops a fraction of the neurons during training to prevent overfitting.\n",
    "     - `MaxPooling2D`: A pooling layer that reduces the spatial dimensions of the input (often used in CNNs for image data).\n",
    "     - `Conv2D`: A convolutional layer, typically used in Convolutional Neural Networks (CNNs) for processing image data.\n",
    "\n",
    "7. **`from tensorflow.keras.models import Model, Sequential`**:\n",
    "   - `Model` and `Sequential` are classes used to define neural network architectures:\n",
    "     - `Sequential`: A linear stack of layers, where each layer has exactly one input and one output.\n",
    "     - `Model`: Used for creating complex models that might have multiple inputs or outputs, often used in more advanced architectures.\n",
    "\n",
    "8. **`from tensorflow.keras.utils import to_categorical`**:\n",
    "   - `to_categorical` is a utility function used to convert class labels (integer encoded) into one-hot encoded vectors. This is commonly used in classification problems.\n",
    "\n",
    "In summary, this set of imports is typically used for building deep learning models, especially convolutional neural networks (CNNs), for tasks such as image classification or other machine learning tasks involving structured or unstructured data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea870f-4969-477c-9a75-883623813abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')/255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ea2ce7-8568-4c08-bf7b-053b245b7e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line of code is transforming the input data `x_train`, which is likely a NumPy array containing pixel values of images (as it's common in image processing tasks). Here's what it does in detail:\n",
    "\n",
    "1. **`x_train.astype('float32')`**:\n",
    "   - This converts the data type of `x_train` to `float32`. This is important because many machine learning frameworks (like TensorFlow) work more efficiently with floating-point numbers, particularly `float32` (32-bit floating point).\n",
    "   - It ensures that the values are in a continuous format, which is necessary for certain mathematical operations involved in training machine learning models.\n",
    "\n",
    "2. **`/255`**:\n",
    "   - This part normalizes the pixel values. Image data is typically represented as integers between 0 and 255 (since each pixel's color value is usually in this range for 8-bit images).\n",
    "   - By dividing the values by 255, each pixel value is scaled down to a range between 0 and 1. This normalization makes it easier for the neural network to learn and process the data effectively, since neural networks generally perform better when inputs are on a smaller, standardized scale.\n",
    "\n",
    "So, this line of code normalizes the pixel values of the training images (in `x_train`), making them between 0 and 1, which is a common preprocessing step in deep learning tasks involving image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25bc138c-7ff2-4eda-941f-5a6666236553",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = train['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d152a4-4e0d-4a2f-9199-21af2499070b",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line of code is extracting the target labels (or output values) for the training data from a pandas DataFrame called `train`.\n",
    "\n",
    "Here’s what happens:\n",
    "\n",
    "1. **`train['label']`**: \n",
    "   - This selects the column named `label` from the `train` DataFrame. The `train` DataFrame is assumed to have a column that contains the target values or labels for each sample in the dataset.\n",
    "   - The `label` column might contain categorical values (like class labels for classification tasks, e.g., 0, 1, 2 for different categories).\n",
    "\n",
    "2. **`.values`**: \n",
    "   - The `.values` attribute converts the selected `label` column into a NumPy array. This is often done because many machine learning libraries (like TensorFlow or Scikit-learn) prefer working with NumPy arrays rather than pandas Series.\n",
    "   - This results in a NumPy array that holds all the label values for the training set.\n",
    "\n",
    "So, after this line, `y_train` will be a NumPy array containing the labels for each sample in the training data. These labels are what the model will try to predict during the training process.\n",
    "\n",
    "If you were working on a classification problem, `y_train` might contain class labels like `0`, `1`, `2`, etc., corresponding to different categories in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9453c3d6-169b-44ec-80a7-63645cb37c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code defines a simple neural network model for multi-class classification:\n",
    "\n",
    "1. **Model Type**: `Sequential`, where layers are added one after another.\n",
    "2. **Layer 1**: `Dense(128)` — a fully connected layer with 128 neurons and ReLU activation.\n",
    "3. **Layer 2**: `Dense(64)` — another fully connected layer with 64 neurons and ReLU activation.\n",
    "4. **Dropout Layer**: `Dropout(0.2)` — randomly drops 20% of the neurons during training to prevent overfitting.\n",
    "5. **Output Layer**: `Dense(10)` — a fully connected layer with 10 neurons and softmax activation, for multi-class classification (10 classes).\n",
    "\n",
    "**Compilation**:\n",
    "- Optimizer: Adam\n",
    "- Loss function: Sparse categorical cross-entropy (for integer-encoded labels)\n",
    "- Metric: Accuracy\n",
    "\n",
    "The `model.summary()` will print the details of the model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a903864b-0eff-4431-9464-f8cb41508753",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = model.fit(x_train, y_train, validation_split= 0.2, batch_size = 128, epochs = 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f480dd4e-d3b8-4d88-9c0a-6e690c8824ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line of code trains the neural network model using the `fit()` method. Here's what each part does:\n",
    "\n",
    "1. **`model.fit(x_train, y_train, ...)`**:\n",
    "   - `x_train`: The training data (input features, typically image data).\n",
    "   - `y_train`: The corresponding target labels (the correct output values, such as class labels).\n",
    "   - The `fit()` method trains the model using this data.\n",
    "\n",
    "2. **`validation_split=0.2`**:\n",
    "   - This specifies that 20% of the training data (`x_train`, `y_train`) will be used as a validation set during training. The model's performance will be evaluated on this set after each epoch to check for overfitting or underfitting.\n",
    "\n",
    "3. **`batch_size=128`**:\n",
    "   - This defines how many samples will be processed together in one batch before updating the model's weights. A batch size of 128 means that 128 samples will be passed through the network at once.\n",
    "\n",
    "4. **`epochs=11`**:\n",
    "   - This specifies that the model will be trained for 11 full passes (epochs) through the entire training dataset.\n",
    "\n",
    "### Summary:\n",
    "- The model is trained on `x_train` and `y_train` for 11 epochs, using 128 samples per batch.\n",
    "- 20% of the training data is used as a validation set to track the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fea03d2-dca5-40cb-9e48-0464b2058252",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = test.drop(['label'], axis = 1).values\n",
    "y_test = test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95053229-00a0-494b-988d-9e947c0fd234",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code prepares the test data:\n",
    "\n",
    "1. **`x_test`**: It drops the `label` column from the `test` DataFrame and stores the remaining feature data (e.g., pixel values) as a NumPy array.\n",
    "2. **`y_test`**: It extracts the `label` column from the `test` DataFrame, which contains the true labels for the test samples, and stores them as a NumPy array.\n",
    "\n",
    "In short:\n",
    "- `x_test`: Input features (e.g., image data).\n",
    "- `y_test`: True labels (the correct outputs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca7363d2-7493-470b-9efe-da5de8455eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = x_test.astype('float32') / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a3029e-7fc2-459b-8c93-6978514191c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line of code normalizes the test data:\n",
    "\n",
    "1. **`x_test.astype('float32')`**: \n",
    "   - Converts the test data into 32-bit floating-point numbers. This is necessary for the model to work efficiently during evaluation.\n",
    "\n",
    "2. **`/ 255`**: \n",
    "   - Divides each pixel value by 255, scaling the values from a range of 0–255 (the usual range for image pixels) to a range of 0–1. This normalization helps the model make better predictions by keeping the input values small and consistent.\n",
    "\n",
    "In short:\n",
    "- The pixel values in `x_test` are converted to numbers between 0 and 1, making them easier for the model to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbf1865f-3c24-4444-bdeb-dfeef503d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f7cb277-82fc-429e-a4a8-073495bbe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "This line of code evaluates the performance of the trained model on the test data:\n",
    "\n",
    "1. **`model.evaluate(x_test, y_test)`**:\n",
    "   - `x_test`: The input features (test images) to the model.\n",
    "   - `y_test`: The true labels (correct answers) for the test data.\n",
    "\n",
    "   The `evaluate()` function computes the **loss** and **accuracy** of the model on the test data. It uses the trained model to make predictions on `x_test` and compares those predictions to the true labels in `y_test`.\n",
    "\n",
    "2. **`test_loss, test_accuracy = ...`**:\n",
    "   - `test_loss`: The value of the loss function (e.g., sparse categorical cross-entropy) after evaluating the model on the test data. A lower loss indicates better model performance.\n",
    "   - `test_accuracy`: The accuracy of the model on the test data, which tells us what percentage of predictions were correct. \n",
    "\n",
    "In short:\n",
    "- The model is tested on `x_test` and `y_test`, and the loss and accuracy are calculated. These values are stored in `test_loss` and `test_accuracy` to evaluate how well the model performs on unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bffc36bc-3e9c-47e0-a383-77482e2039b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "r.history.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76cc18c-10ad-4e48-9ab5-78a749e11ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "The line r.history.keys() is used to access the history of the training process from the fit() method.\n",
    "\n",
    "Here's what it means:\n",
    "\n",
    "    r: This is the variable holding the result of the model.fit() function. When you call model.fit(), it returns a History object, which contains details about the training process for each epoch (such as loss, accuracy, and any other metrics you tracked).\n",
    "\n",
    "    r.history: This is a dictionary that holds the values of the metrics for each epoch. For example, it might contain:\n",
    "        The loss for each epoch (e.g., 'loss').\n",
    "        The accuracy for each epoch (e.g., 'accuracy').\n",
    "        The validation loss and validation accuracy (if validation_split or validation_data was used).\n",
    "\n",
    "    r.history.keys(): This retrieves the keys of the dictionary r.history, which represent the names of the tracked metrics during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3be1be-4e37-4d6b-b6c3-3621812e46e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['accuracy'], label = 'accuracy', color = 'green')\n",
    "plt.plot(r.history['val_accuracy'], label = 'val_accuracy', color = 'red')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71fdee25-f7f7-4e2c-9557-f668c36529c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code creates a plot to visualize the training and validation accuracy over the epochs:\n",
    "\n",
    "1. **`plt.plot(r.history['accuracy'], label='accuracy', color='green')`**:\n",
    "   - This plots the training accuracy (`'accuracy'`) over the epochs.\n",
    "   - `r.history['accuracy']` contains the accuracy values from the training process, and `plt.plot()` will create a line plot for these values.\n",
    "   - The `label='accuracy'` adds a label for this line in the legend, and `color='green'` sets the color of the line to green.\n",
    "\n",
    "2. **`plt.plot(r.history['val_accuracy'], label='val_accuracy', color='red')`**:\n",
    "   - This plots the validation accuracy (`'val_accuracy'`) over the epochs.\n",
    "   - `r.history['val_accuracy']` contains the accuracy values from the validation set during training.\n",
    "   - The `label='val_accuracy'` adds a label for this line in the legend, and `color='red'` sets the color of the line to red.\n",
    "\n",
    "3. **`plt.legend()`**:\n",
    "   - This displays the legend on the plot, showing which line corresponds to `accuracy` (green) and which line corresponds to `val_accuracy` (red).\n",
    "\n",
    "### Purpose:\n",
    "- This plot shows how the accuracy of the model improved during training (for the training data) and how well it performed on the validation data after each epoch. \n",
    "- By comparing the two lines, you can detect overfitting if the training accuracy keeps increasing while the validation accuracy starts decreasing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b85366-904d-40f8-8615-e31e213e4dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(r.history['loss'], label = 'loss', color = 'red')\n",
    "plt.plot(r.history['val_loss'], label = 'val_loss', color = 'blue')\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38a5de1-4d81-400b-9612-630706fc2ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "This code creates a plot to visualize the training and validation loss over the epochs:\n",
    "\n",
    "1. **`plt.plot(r.history['loss'], label='loss', color='red')`**:\n",
    "   - This plots the training loss (`'loss'`) over the epochs.\n",
    "   - `r.history['loss']` contains the loss values from the training process. The `plt.plot()` function creates a line plot of these values.\n",
    "   - `label='loss'` adds a label for this line in the legend, and `color='red'` sets the color of the line to red.\n",
    "\n",
    "2. **`plt.plot(r.history['val_loss'], label='val_loss', color='blue')`**:\n",
    "   - This plots the validation loss (`'val_loss'`) over the epochs.\n",
    "   - `r.history['val_loss']` contains the loss values from the validation set during training.\n",
    "   - `label='val_loss'` adds a label for this line in the legend, and `color='blue'` sets the color of the line to blue.\n",
    "\n",
    "3. **`plt.legend()`**:\n",
    "   - This displays the legend on the plot, showing which line corresponds to `loss` (red) and which line corresponds to `val_loss` (blue).\n",
    "\n",
    "### Purpose:\n",
    "- This plot shows how the loss function (such as cross-entropy loss) decreased during training for both the training data (`loss`) and the validation data (`val_loss`).\n",
    "- By comparing the two lines, you can track how well the model is generalizing to the validation set. If the validation loss starts increasing while the training loss keeps decreasing, it may indicate overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef6fe5a-9e05-49f1-8c10-8d7c80c08c25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
